import os
import random
import numpy as np
import tensorflow as tf
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
from tqdm import tqdm, trange
from collections import defaultdict


def highway_layer(x, activation, carry_bias=-1.0):
    size = x.shape[-1].value
    W_T = tf.Variable(tf.truncated_normal((size, size), stddev=0.1), name='weight_transform')
    b_T = tf.Variable(tf.constant(carry_bias, shape=(size,)), name='bias_transform')
    W = tf.Variable(tf.truncated_normal((size, size), stddev=0.1), name='weight')
    b = tf.Variable(tf.constant(0.1, shape=(size,)), name='bias')
    T = tf.sigmoid(tf.matmul(x, W_T) + b_T, name='transform_gate')
    H = activation(tf.matmul(x, W) + b, name='activation')
    C = tf.subtract(1.0, T, name='carry_gate')
    return tf.add(tf.multiply(H, T), tf.multiply(x, C))


def fingerprint_mols(mols, fp_dim):
    fps = []
    for mol in mols:
        mol = Chem.MolFromSmiles(mol)

        # Necessary for fingerprinting
        Chem.GetSymmSSSR(mol)

        # "When comparing the ECFP/FCFP fingerprints and
        # the Morgan fingerprints generated by the RDKit,
        # remember that the 4 in ECFP4 corresponds to the
        # diameter of the atom environments considered,
        # while the Morgan fingerprints take a radius parameter.
        # So the examples above, with radius=2, are roughly
        # equivalent to ECFP4 and FCFP4."
        # <http://www.rdkit.org/docs/GettingStartedInPython.html>
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=int(fp_dim))
        # fold_factor = fp.GetNumBits()//fp_dim
        # fp = DataStructs.FoldFingerprint(fp, fold_factor)
        fps.append(fp)
    return fps


def fingerprint_reactions(reactions, fp_dim):
    fps = []
    for r in reactions:
        # fp = AllChem.CreateDifferenceFingerprintForReaction(r)
        fp = AllChem.CreateStructuralFingerprintForReaction(r)
        fold_factor = fp.GetNumBits()//fp_dim
        fp = DataStructs.FoldFingerprint(fp, fold_factor)
        fps.append(fp)
    return fps


def train(sess, net, X, y, batch_size=16, epochs=10):
    losses = []
    accuracy = []
    it = trange(epochs)
    X = net.preprocess(X)
    n_steps = int(np.ceil(len(X)/batch_size))
    for e in it:
        # Shuffle
        # p = np.random.permutation(len(X))
        # X, y = X[p], y[p]
        xy = list(zip(X, y))
        random.shuffle(xy)
        X, y = zip(*xy)

        # Iterate batches
        for i in range(n_steps):
            l = i*batch_size
            u = l + batch_size
            X_batch, y_batch = X[l:u], y[l:u]
            _, err, acc = sess.run(
                [net.train_op, net.loss_op, net.acc_op],
                feed_dict={
                    net.keep_prob: 0.4,
                    net.X: X_batch,
                    net.y: y_batch
                }
            )
            losses.append(err)
            accuracy.append(acc)
            it.set_postfix(
                loss=err,
                acc=acc,
                u_loss=np.mean(losses[-10:]) if losses else None,
                u_acc=np.mean(accuracy[-10:]) if accuracy else None)
    return losses


class RolloutPolicyNet:
    def __init__(self, n_rules, fp_dim=8912, k=10):
        self.fp_dim = fp_dim
        self.n_rules = n_rules
        self.X = tf.placeholder(tf.float32, shape=(None, fp_dim))
        self.y = tf.placeholder(tf.float32, shape=(None, n_rules))
        self.keep_prob = tf.placeholder(tf.float32, shape=())

        inp = tf.math.log(self.X+1)
        net = tf.layers.dense(inp, 512, activation=tf.nn.elu)
        net = tf.nn.dropout(net, keep_prob=self.keep_prob)
        net = tf.layers.dense(net, n_rules, activation=None)
        pred = tf.nn.softmax(net)
        self.pred_op = pred
        # self.pred = tf.nn.top_k(pred, k=k)
        self.loss_op = tf.losses.softmax_cross_entropy(self.y, net)
        # self.loss_op = tf.losses.log_loss(self.y, pred)
        self.train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss_op)

        # self.acc_op = tf.metrics.accuracy(self.y, tf.round(pred))
        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(self.y, 1))
        self.acc_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        # all_labels_true = tf.reduce_min(tf.cast(correct_pred, tf.float32), 1)
        # self.acc_op = tf.reduce_mean(all_labels_true)

    def preprocess(self, X):
        # Compute fingerprints
        return fingerprint_mols(X, self.fp_dim)


class ExpansionPolicyNet:
    def __init__(self, n_rules, fp_dim=1e6):
        self.fp_dim = fp_dim
        self.n_rules = n_rules

        # Variance threshold
        # No specific threshold is mentioned in the paper,
        # just that it's used to "remove rare features"
        # threshold = 0 # ?
        # X = np.array(fingerprint_mols(mols, fp_dim))
        # var = np.var(X, axis=0)
        # self.idx = np.where(var > threshold)[0]

        # self.X = tf.placeholder(tf.float32, shape=(None, self.idx.shape[-1]))
        self.X = tf.placeholder(tf.float32, shape=(None, fp_dim))
        self.y = tf.placeholder(tf.float32, shape=(None, n_rules))
        self.k = tf.placeholder(tf.int32, shape=None)
        self.keep_prob = tf.placeholder(tf.float32, shape=())

        inp = tf.math.log(self.X+1)
        net = tf.layers.dense(inp, 512, activation=tf.nn.elu)
        net = tf.nn.dropout(net, keep_prob=self.keep_prob)
        for _ in range(5):
            net = highway_layer(net, activation=tf.nn.elu)
            net = tf.nn.dropout(net, keep_prob=self.keep_prob)

        net = tf.layers.dense(net, n_rules, activation=None)
        pred = tf.nn.sigmoid(net)
        self.pred = tf.nn.top_k(pred, k=self.k)
        # self.loss_op = tf.losses.sigmoid_cross_entropy(self.y, net)
        self.loss_op = tf.losses.log_loss(self.y, pred)
        self.train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss_op)

        pred_labels = tf.equal(tf.round(pred), self.y)
        self.acc_op = tf.reduce_mean(tf.cast(pred_labels, tf.float32))

    def preprocess(self, X):
        # Compute fingerprints
        X = fingerprint_mols(X, self.fp_dim)

        # Apply variance threshold
        # return X[:,self.idx]
        return X


class InScopeFilterNet:
    def __init__(self, product_fp_dim=16384, reaction_fp_dim=2048):
        self.prod_fp_dim = product_fp_dim
        self.react_fp_dim = reaction_fp_dim
        self.X = tf.placeholder(tf.float32, shape=(None, product_fp_dim+reaction_fp_dim))
        self.X_prod = self.X[:,:product_fp_dim]
        self.X_react = self.X[:,product_fp_dim:]
        self.X_react = tf.placeholder(tf.float32, shape=(None, reaction_fp_dim))
        self.y = tf.placeholder(tf.int32, shape=())
        self.keep_prob = tf.placeholder(tf.float32, shape=())

        prod_inp = tf.math.log(self.X_prod+1)
        prod_net = tf.layers.dense(prod_inp, 1024, activation=tf.nn.elu)
        prod_net = tf.nn.dropout(prod_net, keep_prob=self.keep_prob)
        for _ in range(5):
            prod_net = highway_layer(prod_net, activation=tf.nn.elu)

        react_net = tf.layers.dense(self.X_react, 1024, activation=tf.nn.elu)
        net = tf.losses.cosine_distance(prod_net, react_net, axis=0)
        self.pred = tf.nn.sigmoid(net)
        self.loss_op = tf.losses.sigmoid_cross_entropy(self.y, net)
        self.train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss_op)

        self.acc_op = tf.metrics.accuracy(self.y, tf.round(self.pred))

    def preprocess(self, X):
        # Compute fingerprints
        prod_mols, react_mols = zip(*X)
        prod_fps = fingerprint_mols(prod_mols, self.prod_fp_dim)
        react_fps = fingerprint_reactions(react_mols, self.react_fp_dim)
        return np.hstack([prod_fps, react_fps])


if __name__ == '__main__':
    print('Loading data...')

    prod_to_rules = defaultdict(set)
    with open('data/templates.dat', 'r') as f:
        for l in tqdm(f, desc='products'):
            rule, prod = l.strip().split('\t')
            prod_to_rules[prod].add(rule)

    rollout_rules = {}
    with open('data/rollout.dat', 'r') as f:
        for i, l in tqdm(enumerate(f), desc='rollout'):
            rule = l.strip()
            rollout_rules[rule] = i

    expansion_rules = {}
    with open('data/expansion.dat', 'r') as f:
        for i, l in tqdm(enumerate(f), desc='expansion'):
            rule = l.strip()
            expansion_rules[rule] = i

    rollout = RolloutPolicyNet(n_rules=len(rollout_rules))
    # expansion = ExpansionPolicyNet(n_rules=len(expansion_rules))
    # filter = InScopeFilterNet()

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)

    save_path = 'model'
    ckpt_path = os.path.join(save_path, 'model.ckpt')
    saver = tf.train.Saver()
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    saver.restore(sess, ckpt_path)

    # Rollout training
    print('Rollout training...')
    X, y = [], []
    for prod, rules in tqdm(prod_to_rules.items(), desc='data prep'):
        rules = [r for r in rules if r in rollout_rules]
        if not rules: continue

        for r in rules:
            labels = np.zeros(len(rollout_rules), dtype=np.float32)
            id = rollout_rules[r]
            labels[id] = 1.
            y.append(labels)
            X.append(prod)
    print('Training size:', len(X))

    # Check
    # X = rollout.preprocess(X[20:30])
    # y_pred = sess.run(rollout.pred_op, feed_dict={
    #     rollout.keep_prob: 1.,
    #     rollout.X: X
    # })
    # print(np.argmax(y_pred, 1))
    # print(np.argmax(y[20:30], 1))

    # train(sess, rollout, X, y, batch_size=128, epochs=100)
    # saver.save(sess, ckpt_path)

    # print('Expansion training...')
    # X, y = [], []
    # for prod, rules in tqdm(prod_to_rules.items(), desc='data prep'):
    #     rules = [r for r in rules if r in expansion_rules]
    #     if not rules: continue

    #     # Multi-label
    #     labels = np.zeros(len(expansion_rules), dtype=np.float32)
    #     for r in rules:
    #         id = expansion_rules[r]
    #         labels[id] = 1.
    #     y.append(labels)
    #     X.append(prod)
    # train(sess, expansion, X, y, batch_size=32, epochs=10)
    # saver.save(sess, ckpt_path)

    # # InScopeFilter training
    # X, y = [], []
    # exists = set()
    # for prod, rules in tqdm(prod_to_rules.items(), desc='data prep'):
    #     rules = [r for r in rules if r in expansion_rules]
    #     if not rules: continue

    #     for r in rules:
    #         y.append(1.)
    #         X.append((prod, r))
    #         exists.add('{}_{}'.format(prod, r))

    # # Generate negative examples
    # target_size = len(X) * 2
    # pbar = tqdm(total=target_size, desc='data prep (negative)')
    # while len(X) < target_size:
    #     prod = random.choice(prod_to_rules.keys())
    #     rule = random.choice(expansion_rules)

    #     key = '{}_{}'.format(prod, r)
    #     if key in exists:
    #         continue
    #     else:
    #         y.append(0.)
    #         X.append((prod, rule))
    #         pbar.update(1)
    # pbar.close()

    # train(sess, filter, X, y, batch_size=32, epochs=10)
    # saver.save(sess, ckpt_path)