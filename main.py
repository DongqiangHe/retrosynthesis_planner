import os
import numpy as np
import tensorflow as tf
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
from tqdm import tqdm, trange


def highway_layer(x, activation, carry_bias=-1.0):
    size = x.shape[-1].value
    W_T = tf.Variable(tf.truncated_normal((size, size), stddev=0.1), name='weight_transform')
    b_T = tf.Variable(tf.constant(carry_bias, shape=(size,)), name='bias_transform')
    W = tf.Variable(tf.truncated_normal((size, size), stddev=0.1), name='weight')
    b = tf.Variable(tf.constant(0.1, shape=(size,)), name='bias')
    T = tf.sigmoid(tf.matmul(x, W_T) + b_T, name='transform_gate')
    H = activation(tf.matmul(x, W) + b, name='activation')
    C = tf.subtract(1.0, T, name='carry_gate')
    return tf.add(tf.multiply(H, T), tf.multiply(x, C), 'y')


def fingerprint_mols(mols, fp_dim):
    fps = []
    for mol in mols:
        # "When comparing the ECFP/FCFP fingerprints and
        # the Morgan fingerprints generated by the RDKit,
        # remember that the 4 in ECFP4 corresponds to the
        # diameter of the atom environments considered,
        # while the Morgan fingerprints take a radius parameter.
        # So the examples above, with radius=2, are roughly
        # equivalent to ECFP4 and FCFP4."
        # <http://www.rdkit.org/docs/GettingStartedInPython.html>
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=int(fp_dim))
        # fold_factor = fp.GetNumBits()//fp_dim
        # fp = DataStructs.FoldFingerprint(fp, fold_factor)
        fps.append(fp)
    return fps

def fingerprint_mol(mol, fp_dim):
    return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=int(fp_dim))

def fingerprint_reactions(reactions, fp_dim):
    fps = []
    for r in reactions:
        # fp = AllChem.CreateDifferenceFingerprintForReaction(r)
        fp = AllChem.CreateStructuralFingerprintForReaction(r)
        fold_factor = fp.GetNumBits()//fp_dim
        fp = DataStructs.FoldFingerprint(fp, fold_factor)
        fps.append(fp)
    return fps


def train(sess, net, X, y, batch_size=16, epochs=10):
    losses = []
    it = trange(epochs)
    X = net.preprocess(X)
    n_steps = int(np.ceil(len(X)/batch_size))
    for e in it:
        # Shuffle
        p = np.random.permutation(len(X))
        X, y = X[p], y[p]

        # Iterate batches
        for i in range(n_steps):
            l = i*batch_size
            u = l + batch_size
            X_batch, y_batch = X[l:u], y[l:u]
            _, err = sess.run(
                [net.train_op, net.loss_op],
                feed_dict={
                    net.training: True,
                    net.X: X_batch,
                    net.y: y_batch
                }
            )
            losses.append(err)
            it.set_postfix(
                loss=err,
                mean_loss=np.mean(losses[-10:]) if losses else None)
    return losses


class RolloutPolicyNet:
    def __init__(self, fp_dim=8912, n_rules=17134, k=10):
        self.fp_dim = fp_dim
        self.n_rules = n_rules
        self.X = tf.placeholder(tf.float32, shape=(None, fp_dim))
        self.y = tf.placeholder(tf.int32, shape=(None, n_rules))
        self.training = tf.placeholder(tf.bool, shape=None)

        inp = tf.math.log(self.X+1)
        net = tf.layers.dense(inp, 512, activation=tf.nn.elu)
        net = tf.nn.dropout(net, keep_prob=0.4, training=self.training)
        net = tf.layers.dense(net, n_rules, activation=None)
        pred = tf.nn.softmax(net)
        self.pred = tf.nn.top_k(pred, k=k)
        self.loss_op = tf.losses.softmax_cross_entropy(self.y, net)
        self.train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss_op)

    def preprocess(self, X):
        # Compute fingerprints
        return fingerprint_mols(X, self.fp_dim)


class ExpansionPolicyNet:
    def __init__(self, mols, fp_dim=1e6, n_rules=301671):
        self.fp_dim = fp_dim
        self.n_rules = n_rules

        # Variance threshold
        # No specific threshold is mentioned in the paper,
        # just that it's used to "remove rare features"
        threshold = 0 # ?
        X = np.array(fingerprint_mols(mols, fp_dim))
        var = np.var(X, axis=0)
        self.idx = np.where(var > threshold)[0]

        self.X = tf.placeholder(tf.float32, shape=(None, self.idx.shape[-1]))
        self.y = tf.placeholder(tf.int32, shape=(None, n_rules))
        self.k = tf.placeholder(tf.int32, shape=None)
        self.training = tf.placeholder(tf.bool, shape=None)

        inp = tf.math.log(self.X+1)
        net = tf.layers.dense(inp, 512, activation=tf.nn.elu)
        net = tf.nn.dropout(net, keep_prob=0.4, training=self.training)
        for _ in range(5):
            net = highway_layer(net, activation=tf.nn.elu)
            net = tf.nn.dropout(net, keep_prob=0.1, training=self.training)

        net = tf.layers.dense(net, n_rules, activation=None)
        pred = tf.nn.softmax(net)
        self.pred = tf.nn.top_k(pred, k=self.k)
        self.loss_op = tf.losses.softmax_cross_entropy(self.y, net)
        self.train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss_op)

    def preprocess(self, X):
        # Compute fingerprints
        X = fingerprint_mols(X, self.fp_dim)

        # Apply variance threshold
        return X[:,self.idx]


class InScopeFilterNet:
    def __init__(self, product_fp_dim=16384, reaction_fp_dim=2048):
        self.prod_fp_dim = product_fp_dim
        self.react_fp_dim = reaction_fp_dim
        self.X = tf.placeholder(tf.float32, shape=(None, product_fp_dim+reaction_fp_dim))
        self.X_prod = self.X[:,:product_fp_dim]
        self.X_react = self.X[:,product_fp_dim:]
        self.X_react = tf.placeholder(tf.float32, shape=(None, reaction_fp_dim))
        self.y = tf.placeholder(tf.int32, shape=())
        self.training = tf.placeholder(tf.bool, shape=None)

        prod_inp = tf.math.log(self.X_prod+1)
        prod_net = tf.layers.dense(prod_inp, 1024, activation=tf.nn.elu)
        prod_net = tf.nn.dropout(prod_net, keep_prob=0.3, training=self.training)
        for _ in range(5):
            prod_net = highway_layer(prod_net, activation=tf.nn.elu)

        react_net = tf.layers.dense(self.X_react, 1024, activation=tf.nn.elu)
        net = tf.losses.cosine_distance(prod_net, react_net, axis=0)
        self.pred = tf.nn.sigmoid(net)
        self.loss_op = tf.losses.sigmoid_cross_entropy(self.y, net)
        self.train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss_op)

    def preprocess(self, X):
        # Compute fingerprints
        prod_mols, react_mols = zip(*X)
        prod_fps = fingerprint_mols(prod_mols, self.prod_fp_dim)
        react_fps = fingerprint_reactions(react_mols, self.react_fp_dim)
        return np.hstack([prod_fps, react_fps])


if __name__ == '__main__':
    data = []
    with open('data/uspto/reactions.rsmi', 'r') as f:
        for i, line in tqdm(enumerate(f)):
            line = line.strip().split()[0]
            reaction = AllChem.ReactionFromSmarts(line, useSmiles=True)
            for p in reaction.GetProducts():
                data.append((p, reaction))
                # Necessary for fingerprinting
                Chem.GetSymmSSSR(p)
            if i > 10000:
                break

    import ipdb; ipdb.set_trace()

    rollout = RolloutPolicyNet()
    expansion = ExpansionPolicyNet()
    filter = InScopeFilterNet()

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)

    save_path = 'model'
    ckpt_path = os.path.join(save_path, 'model.ckpt')
    saver = tf.train.Saver()
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    else:
        saver.restore(sess, ckpt_path)

    saver.save(sess, ckpt_path)